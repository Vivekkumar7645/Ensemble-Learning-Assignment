{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN7LsdLyEkDmlW6cUL82xgR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##Q 1. Can we use Bagging for regression problems?\n","**Ans** - Bagging can be used for regression problems. While it is commonly associated with classification tasks, it is equally effective for regression.\n","\n","**Bagging Works in Regression**\n","1. It creates multiple subsets of the training data by bootstrapping.\n","2. A base regressor is trained on each subset.\n","3. The predictions from all regressors are averaged to get the final output.\n","\n","**Use Bagging for Regression**\n","* Reduces Variance: Especially useful for models like Decision Trees, which tend to overfit.\n","* Improves Stability: The averaging of multiple models results in more robust predictions.\n","* Handles Noisy Data Well: Since multiple models contribute to the final result, the impact of noise is reduced.\n","\n","**Popular Bagging-Based Regression Models**\n","1. Bagging Regressor - Implemented in sklearn.ensemble.BaggingRegressor\n","2. Random Forest Regressor - A special case of Bagging with Decision Trees (sklearn.ensemble.RandomForestRegressor)\n","\n","**Example in Python**"],"metadata":{"id":"gG6rauR8DdHO"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","base_model = DecisionTreeRegressor()\n","\n","bagging_regressor = BaggingRegressor(base_model, n_estimators=50, random_state=42)\n","bagging_regressor.fit(X_train, y_train)\n","\n","y_pred = bagging_regressor.predict(X_test)\n","\n","mse = mean_squared_error(y_test, y_pred)\n","print(\"Mean Squared Error:\", mse)"],"metadata":{"id":"dX5GksmYKMqm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Q 2. What is the difference between multiple model training and single model training?\n","**Ans** - The difference between multiple model training and single model training lies in how many models are trained and how predictions are made.\n","\n","**1. Single Model Training**\n","* Definition\n","  * In single model training, we train only one model on the dataset.\n","  * The model learns from the entire dataset and makes predictions based on its learned patterns.\n","\n","**Advantages**\n","* Simple and Efficient - Requires fewer computational resources.\n","* Easier to Interpret - Good for understanding the decision-making process.\n","* Less Training Time - Only one model needs to be trained.\n","\n","**Disadvantages**\n","* May Overfit or Underfit - A single model might not generalize well.\n","* Less Robust - Errors from one model cannot be corrected by others.\n","\n","**Example**\n","\n","Training a single Decision Tree, Linear Regression, or Neural Network on a dataset."],"metadata":{"id":"z275tCK7DdnW"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","\n","model = DecisionTreeRegressor()\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)"],"metadata":{"id":"bOGbfYOrK813"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2. Multiple Model Training**\n","\n","Definition\n","* In multiple model training, we train several models on the dataset.\n","* The final prediction is based on the combination of these models.\n","\n","**Types of Multiple Model Training**\n","1. Bagging - Train multiple models on different subsets of data (e.g., Random Forest).\n","2. Boosting - Train models sequentially, where each model learns from the previous model's mistakes (e.g., AdaBoost, XGBoost).\n","3. Stacking - Train multiple models and use another model to combine their predictions.\n","4. Ensemble Voting - Combine predictions from different models and take the majority vote or average.\n","\n","**Advantages**\n","* More Robust - Reduces variance and improves generalization.\n","* Handles Complex Problems Better - Can capture more patterns in the data.\n","* Reduces Overfitting - Especially useful for high-variance models.\n","\n","**Disadvantages**\n","* More Computationally Expensive - Requires training multiple models.\n","* Harder to Interpret - Difficult to understand why a prediction was made.\n","\n","**Example (Random Forest - Bagging)**"],"metadata":{"id":"zK-IWEN7LAFr"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","\n","model = RandomForestRegressor(n_estimators=50)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)"],"metadata":{"id":"LRi6y7WJgq0k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Example (Boosting - XGBoost)"],"metadata":{"id":"3EwDuHNAguq8"}},{"cell_type":"code","source":["from xgboost import XGBRegressor\n","\n","model = XGBRegressor(n_estimators=100)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)"],"metadata":{"id":"rBrdbO5Ngx6a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Differences**\n","\n","|Feature\t|Single Model Training\t|Multiple Model Training|\n","|-|||\n","|Number of Models\t|One model\t|Multiple models|\n","|Complexity\t|Simple\t|More complex|\n","|Computational Cost\t|Low\t|High|\n","|Overfitting\t|More likely\t|Reduced risk|\n","|Interpretability\t|Easier to understand\t|Harder to interpret|\n","|Performance\t|May be limited\t|Usually better|"],"metadata":{"id":"syBz24iNg0rE"}},{"cell_type":"markdown","source":["##Q 3. Explain the concept of feature randomness in Random Forest?\n","**Ans** - **Feature Randomness in Random Forest**\n","\n","Feature randomness in Random Forest refers to how the algorithm selects a random subset of features at each split in a decision tree. This randomness helps in reducing overfitting and improving model robustness.\n","\n","**Feature Randomness Work**\n","1. Bootstrapping the Data - Random Forest first creates multiple subsets of the training data using bootstrap sampling.\n","2. Random Feature Selection at Splits - Instead of considering all features at each split, Random Forest randomly selects a subset of features and picks the best split among them.\n","3. Tree Independence - Each tree in the Random Forest is trained on a different bootstrap sample and different feature subsets, making them diverse.\n","4. Final Prediction - For classification, the majority vote is taken across trees, while for regression, the average of predictions is taken.\n","\n","**Use of Feature Randomness**\n","* Reduces Overfitting - Unlike regular decision trees, where the same dominant features may be used repeatedly, Random Forest prevents over-reliance on any single feature.\n","* Improves Diversity in Trees - Different trees use different feature combinations, making the ensemble stronger.\n","* Increases Model Generalization - Since trees are trained on different subsets of features, they generalize better to unseen data.\n","\n","**Feature Randomness in Sklearn's RandomForestRegressor**\n","\n","In RandomForestRegressor, the parameter max_features controls feature randomness.\n","\n","|max_features Value\t|Description|\n","|-||\n","|\"sqrt\" (default for classification)\t|Selects √N features at each split.|\n","|\"log2\"\t|Selects log₂(N) features.|\n","|None (or N)\t|Uses all features (like a regular decision tree).|\n","|int (e.g., 5)\t|Uses a fixed number of features per split.|\n","|float (e.g., 0.5)\t|Uses a fraction of total features per split.|\n","\n","**Example in Python**"],"metadata":{"id":"gmQv-hcKDeC3"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","\n","X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n","\n","model = RandomForestRegressor(n_estimators=100, max_features=3, random_state=42)\n","model.fit(X, y)\n","\n","print(model.feature_importances_)"],"metadata":{"id":"D0n4sjlriEQ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Q 4. What is OOB (Out-of-Bag) Score?\n","**Ans** - Out-of-Bag Score is a built-in cross-validation technique used in Random Forest to estimate its performance without needing a separate validation set.\n","\n","**OOB Working**\n","1. Bootstrap Sampling: Each tree in the Random Forest is trained on a random subset of the dataset, selected through bootstrapping.\n","2. OOB Samples: Since bootstrapping leaves out around 37% of the training data, these left-out samples are called Out-of-Bag samples.\n","3. OOB Prediction: Each OOB sample is predicted using only the trees that did not see it during training.\n","4. OOB Score Calculation: The final OOB score is computed as the average accuracy or R² score over all OOB samples.\n","\n","**Use of OOB Score**\n","* No Need for Extra Validation Data - Saves data for training.\n","* Unbiased Performance Estimation - Uses unseen data to evaluate model performance.\n","* Faster than Cross-Validation - Provides a built-in validation mechanism without needing k-fold splits.\n","\n","**OOB Score in Scikit-Learn**\n","\n","In RandomForestRegressor or RandomForestClassifier, setting oob_score=True enables OOB evaluation.\n","\n","**Example (Regression)**"],"metadata":{"id":"fMgL_kUfDeYe"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","\n","X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n","\n","rf = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=42)\n","rf.fit(X, y)\n","\n","print(\"OOB Score (R²):\", rf.oob_score_)"],"metadata":{"id":"pw-4YFQRinbU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Example (Classification)**"],"metadata":{"id":"RhqnlUPFirVU"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_classification\n","\n","X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n","\n","rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n","rf.fit(X, y)\n","\n","print(\"OOB Score (Accuracy):\", rf.oob_score_)"],"metadata":{"id":"3BcrJnCRiuRE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* OOB score is like a built-in cross-validation in Random Forest.\n","* It estimates model performance without requiring a separate test set.\n","* Useful for reducing overfitting and making sure the model generalizes well."],"metadata":{"id":"U6-7s86gixRd"}},{"cell_type":"markdown","source":["##Q 5. How can you measure the importance of features in a Random Forest model?\n","**Ans** - **Measuring Feature Importance in Random Forest**\n","\n","Feature importance helps determine which features contribute the most to a Random Forest model’s predictions. There are two main ways to measure feature importance:\n","\n","**1. Mean Decrease in Impurity - Gini Importance**\n","* This method measures how much a feature reduces impurity across all trees in the forest.\n","* Features that split the data more effectively get higher importance scores.\n","\n","**Getting MDI Feature Importance in Scikit-Learn**"],"metadata":{"id":"tzfFytl6DevP"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n","\n","rf = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf.fit(X, y)\n","\n","feature_importance = rf.feature_importances_\n","\n","plt.bar(range(X.shape[1]), feature_importance)\n","plt.xlabel(\"Feature Index\")\n","plt.ylabel(\"Importance Score\")\n","plt.title(\"Feature Importance in Random Forest\")\n","plt.show()"],"metadata":{"id":"mXh8r_9EjGoz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Pros & Cons**\n","* Fast and efficient\n","* Easy to interpret\n","* Biased towards high-cardinality features\n","\n","**2. Mean Decrease in Accuracy - Permutation Importance**\n","* This method randomly shuffles each feature and observes how much the model performance drops.\n","* If shuffling a feature leads to a significant drop in accuracy, it means that feature is important.\n","\n","**Computation of Permutation Importance in Scikit-Learn**"],"metadata":{"id":"DNFqb5j5jMUE"}},{"cell_type":"code","source":["from sklearn.inspection import permutation_importance\n","\n","perm_importance = permutation_importance(rf, X, y, n_repeats=10, random_state=42)\n","\n","plt.bar(range(X.shape[1]), perm_importance.importances_mean)\n","plt.xlabel(\"Feature Index\")\n","plt.ylabel(\"Permutation Importance\")\n","plt.title(\"Permutation Feature Importance\")\n","plt.show()"],"metadata":{"id":"mEYxfk8ujY0T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Pros & Cons**\n","* More reliable than MDI\n","* Works with any model, not just Random Forest\n","* Computationally expensive\n","\n","**Comparison of MDI vs. MDA**\n","\n","|Method\t|How It Works\t|Pros\t|Cons|\n","|-||||\n","|MDI (Default Feature Importance in Sklearn)\t|Measures impurity reduction (Gini/entropy/variance)\t|Fast, easy to compute\t|Biased towards features with more categories|\n","|MDA (Permutation Importance)\t|Shuffles features and checks accuracy drop\t|More reliable, works with any model\t|Slower, computationally expensive|"],"metadata":{"id":"lWPZSYkCEDkv"}},{"cell_type":"markdown","source":["##Q 6. Explain the working principle of a Bagging Classifier.\n","**Ans** - A Bagging Classifier is an ensemble learning method that improves the accuracy and robustness of machine learning models by training multiple models on different subsets of the data and combining their predictions.\n","\n","**Working of Bagging Classifier**\n","\n","Step-by-Step Process:\n","1. Bootstrap Sampling\n","  * The dataset is randomly sampled with replacement to create multiple subsets.\n","  * Each subset is slightly different but maintains the same overall distribution.\n","2. Train Multiple Base Models\n","  * A separate model is trained on each subset.\n","  * Each model learns slightly different patterns due to different training data.\n","\n","3. Make Predictions\n","  * Each trained model independently makes predictions on new data.\n","\n","4. Aggregate Predictions\n","  * For classification, the final prediction is determined by majority voting.\n","  * For regression, the final prediction is the average of all model predictions.\n","\n","**Use of Bagging Classifier**\n","* Reduces Overfitting - Training on different data subsets ensures better generalization.\n","* Decreases Variance - Combining multiple models reduces the impact of individual model errors.\n","* Handles Noisy Data Well - Averaging multiple predictions cancels out noise.\n","* Works Well with High-Variance Models - Great for Decision Trees, which tend to overfit.\n","\n","**Example: Bagging Classifier in Python**\n","\n","Using Scikit-Learn to implement a Bagging Classifier with Decision Trees:"],"metadata":{"id":"-snV17NFDfF2"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","base_model = DecisionTreeClassifier()\n","\n","bagging_clf = BaggingClassifier(base_model, n_estimators=50, random_state=42)\n","bagging_clf.fit(X_train, y_train)\n","\n","y_pred = bagging_clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(\"Bagging Classifier Accuracy:\", accuracy)"],"metadata":{"id":"8_nYkPFaJzKc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Parameters of BaggingClassifier**\n","\n","|Parameter\t|Description|\n","|-||\n","|base_estimator\t|The weak learner|\n","|n_estimators\t|Number of models in the ensemble|\n","|max_samples\t|Number of samples per base model (default = all)|\n","| max features\t| Number of features used per model\n","|bootstrap\t|Whether to sample with replacement|\n","|oob_score\t|Use Out-of-Bag samples for evaluation (default = False)|\n","\n","**Comparison: Bagging vs. Boosting**\n","\n","|Feature\t|Bagging\t|Boosting|\n","|-|||\n","|Training\t|Models train independently\t|Models train sequentially|\n","|Goal\t|Reduce variance\t|Reduce bias|\n","|Weak Learners\t|Usually high-variance models (e.g., Decision Trees)\t|Weak models (e.g., Shallow Trees)|\n","|Final Prediction\t|Majority vote (classification) / Average (regression)\t|Weighted combination of models|\n","|Common Example\t|Random Forest\t|AdaBoost, XGBoost|"],"metadata":{"id":"LGDxHD0LJ387"}},{"cell_type":"markdown","source":["##Q 7. How do you evaluate a Bagging Classifier's performance?\n","**Ans** - Evaluating a Bagging Classifier involves assessing its accuracy, robustness, and generalization ability.\n","\n","**1. Train-Test Split Evaluation**\n","* Split the dataset into training and testing sets.\n","* Train the Bagging Classifier on the training set.\n","* Measure performance on the test set using accuracy, precision, recall, F1-score, etc.\n","\n","**Example (Using Accuracy for Evaluation)**"],"metadata":{"id":"YzbisnBqDfdO"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","bagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n","bagging_clf.fit(X_train, y_train)\n","\n","y_pred = bagging_clf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Test Accuracy:\", accuracy)"],"metadata":{"id":"kn9PyO1GLIP1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Pros: Fast and simple\n","* Cons: Performance may depend on how the data is split\n","\n","**2. Cross-Validation**\n","* K-Fold Cross-Validation splits data into K subsets.\n","* The model is trained K times, each time using a different subset as a test set.\n","* The final performance is the average of all K iterations.\n","\n","**Example (Using Cross-Validation for Evaluation)**"],"metadata":{"id":"933UcXRoLNds"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","\n","cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring=\"accuracy\")\n","\n","print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n","print(\"Mean Accuracy:\", cv_scores.mean())"],"metadata":{"id":"yyvEj1H3Lc6P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Pros: More reliable, less dependent on a single train-test split\n","* Cons: Computationally expensive\n","\n","**3. Out-of-Bag Score (Built-in Validation)**\n","* In Bagging, each tree is trained on a random subset of data.\n","* The left-out samples (Out-of-Bag samples) can be used as a validation set.\n","* This provides an unbiased performance estimate without needing cross-validation.\n","\n","**Example (Using OOB Score)**"],"metadata":{"id":"iYzCwUqMLkRo"}},{"cell_type":"code","source":["bagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, oob_score=True, random_state=42)\n","bagging_clf.fit(X_train, y_train)\n","\n","print(\"OOB Score:\", bagging_clf.oob_score_)"],"metadata":{"id":"nB8DlQXyLxCj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Pros: No need for a separate validation set, efficient\n","* Cons: Less reliable for small datasets\n","\n","**4. Classification Metrics (Precision, Recall, F1-Score, AUC-ROC)**\n","\n","For imbalanced datasets, accuracy alone is not enough.Use\n","* Precision (TP / (TP + FP)) - How many positive predictions were actually correct?\n","* Recall (TP / (TP + FN)) - How many actual positives were correctly identified?\n","* F1-Score - Harmonic mean of precision and recall.\n","* ROC-AUC Score - Measures how well the model separates classes.\n","\n","**Example (Using Precision, Recall, F1-Score, and AUC)**"],"metadata":{"id":"QvURcSLvL1Yj"}},{"cell_type":"code","source":["from sklearn.metrics import classification_report, roc_auc_score\n","\n","print(classification_report(y_test, y_pred))\n","\n","y_prob = bagging_clf.predict_proba(X_test)[:, 1]\n","roc_auc = roc_auc_score(y_test, y_prob)\n","print(\"ROC-AUC Score:\", roc_auc)"],"metadata":{"id":"DnANYPnrMEzL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Pros: Useful for imbalanced datasets\n","* Cons: More complex interpretation\n","\n","**5. Computational Performance**\n","* Measure training time to ensure efficiency.\n","* Measure prediction speed if real-time inference is required.\n","\n","**Example (Measuring Training & Prediction Time)**"],"metadata":{"id":"rR3R0hxUMJUr"}},{"cell_type":"code","source":["import time\n","\n","start_time = time.time()\n","bagging_clf.fit(X_train, y_train)\n","training_time = time.time() - start_time\n","\n","start_time = time.time()\n","y_pred = bagging_clf.predict(X_test)\n","prediction_time = time.time() - start_time\n","\n","print(\"Training Time:\", training_time, \"seconds\")\n","print(\"Prediction Time:\", prediction_time, \"seconds\")"],"metadata":{"id":"787WQcJLMZLs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Pros: Important for large datasets and real-time applications\n","* Cons: Not directly related to accuracy\n","\n","**Summary Evaluation Methods**\n","\n","|Method\t|What It Measures\t|Best For|\n","|-|||\n","|Train-Test Split\t|Accuracy on a single test set\t|Quick evaluation|\n","|Cross-Validation\t|Accuracy across multiple test sets\t|More reliable performance estimate|\n","|OOB Score\t|Accuracy using unused bootstrap samples\t|Avoiding extra validation sets|\n","|Precision, Recall, F1-Score\t|Model performance on imbalanced data\t|Handling class imbalance|\n","|ROC-AUC Score\t|Ability to distinguish between classes\t|Binary classification problems|\n","|Training & Prediction Time\t|Computational efficiency\t|Large-scale or real-time applications|"],"metadata":{"id":"qwfsPk82McrL"}},{"cell_type":"markdown","source":["##Q 8. How does a Bagging Regressor work?\n","**Ans** - A Bagging Regressor is an ensemble learning method that improves the accuracy and stability of regression models by averaging multiple predictions from different models trained on random subsets of data.\n","\n","**Working of Bagging Regressor**\n","\n","Step-by-Step Process:\n","1. Bootstrap Sampling\n","  * The dataset is randomly sampled with replacement to create multiple training subsets.\n","  * Each subset is slightly different but retains the overall distribution.\n","2. Train Multiple Base Models\n","  * A separate base model is trained on each subset.\n","  * Each model learns slightly different patterns due to different training data.\n","3. Make Predictions\n","  * Each trained model independently makes predictions on new data.\n","4. Aggregate Predictions\n","  * The final prediction is the average of all individual model predictions.\n","  * This reduces variance and improves generalization.\n","\n","          ŷ = 1/N∑ᴺᵢ₌₁ ŷᵢ\n","where N is the number of base models and ŷᵢ is the prediction from the i-th model.\n","\n","**Use of Bagging Regressor**\n","* Reduces Overfitting - Prevents a single model from capturing noise.\n","* Decreases Variance - Aggregating predictions smooths out extreme predictions.\n","* Improves Stability - Works well with high-variance models like Decision Trees.\n","* Handles Noisy Data Well - Averaging cancels out individual model errors.\n","\n","**Example: Bagging Regressor in Python**\n","\n","Using Scikit-Learn to implement a Bagging Regressor with Decision Trees:"],"metadata":{"id":"G27wnvhnDfz4"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","bagging_reg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50, random_state=42)\n","bagging_reg.fit(X_train, y_train)\n","\n","y_pred = bagging_reg.predict(X_test)\n","\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R² Score:\", r2)"],"metadata":{"id":"mR6wOZcxQbbM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Parameters of BaggingRegressor**\n","\n","|Parameter\t|Description|\n","|-||\n","|base_estimator\t|The weak learner (e.g., DecisionTreeRegressor)|\n","|n_estimators\t|Number of models in the ensemble|\n","|max_samples\t|Number of samples per base model (default = all)|\n","|max_features\t|Number of features used per model|\n","|bootstrap\t|Whether to sample with replacement (default = True)|\n","|oob_score\t|Use Out-of-Bag samples for evaluation (default = False)|\n","\n","**Evaluating a Bagging Regressor**\n","\n","To assess performance, we use\n","1. Mean Squared Error - Measures average squared error.\n","2. R² Score (Coefficient of Determination) - Measures how well predictions fit actual values.\n","3. Cross-Validation - More reliable performance estimate.\n","4. OOB Score - Internal validation method.\n","\n","**Example: Using OOB Score**"],"metadata":{"id":"rX6E681LQgWx"}},{"cell_type":"code","source":["bagging_reg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50, oob_score=True, random_state=42)\n","bagging_reg.fit(X_train, y_train)\n","\n","print(\"OOB Score (R²):\", bagging_reg.oob_score_)"],"metadata":{"id":"Tq1ErTFBREC7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* OOB Score provides an unbiased estimate of model performance.\n","\n","**Comparison: Bagging vs. Boosting**\n","\n","|Feature\t|Bagging\t|Boosting|\n","|-|||\n","|Training\t|Models train independently\t|Models train sequentially|\n","|Goal\t|Reduce variance\t|Reduce bias|\n","|Weak Learners\t|Usually high-variance models (e.g., Decision Trees)\t|Weak models (e.g., Shallow Trees)|\n","|Final Prediction\t|Averaging\t|Weighted combination of models|\n","|Common Example\t|Random Forest\t|AdaBoost, XGBoost|"],"metadata":{"id":"cixxoka-RHc7"}},{"cell_type":"markdown","source":["##Q 9. What is the main advantage of ensemble techniques?\n","**Ans** - The primary advantage of ensemble techniques is that they combine multiple models to improve overall performance, resulting in a model that is more accurate, robust, and generalizable compared to individual models.\n","\n","**Advantages of Ensemble Techniques**\n","1. Higher Accuracy\n","  * Combining multiple weak models leads to better predictions.\n","    * Example: Random Forest outperforms individual Decision Trees.\n","2. Reduces Overfitting\n","  * Individual models may overfit the training data.\n","  * Ensembles like Bagging (e.g., Random Forest) reduce variance and improve generalization.\n","3. Reduces Bias & Variance Trade-off\n","  * Bagging reduces variance.\n","  * Boosting reduces bias.\n","    * Example: Gradient Boosting minimizes both.\n","4. More Stable & Robust Predictions\n","  * Small changes in data do not significantly affect performance.\n","    * Example: Voting and Stacking methods increase stability.\n","5. Works Well with Complex Data\n","  * Handles non-linear relationships and high-dimensional datasets effectively.\n","    * Example: XGBoost for structured data.\n","6. Better Handling of Noisy Data 🔊\n","  * Aggregation cancels out individual model errors.\n","    * Example: Bagging smooths out extreme predictions.\n","7. Flexibility in Model Selection 🔧\n","  * Can use heterogeneous models (e.g., Decision Trees, SVM, Neural Networks) in Stacking.\n","\n","**Example: Ensemble Learning in Action**\n","\n","Comparing Decision Tree vs. Random Forest"],"metadata":{"id":"dh1bzUnUDgKm"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","dt = DecisionTreeClassifier(random_state=42)\n","dt.fit(X_train, y_train)\n","y_pred_dt = dt.predict(X_test)\n","\n","rf = RandomForestClassifier(n_estimators=50, random_state=42)\n","rf.fit(X_train, y_train)\n","y_pred_rf = rf.predict(X_test)\n","\n","print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))"],"metadata":{"id":"u0QdMa57sDdd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Q 10. What is the main challenge of ensemble methods?\n","**Ans** - Ensemble methods improve accuracy and generalization, they come with certain challenges:\n","\n","**1. Higher Computational Cost**\n","  * Training multiple models requires more time and resources compared to a single model.\n","  * Complex ensembles like Random Forest or Boosting can be slow.\n","  * Example: XGBoost and Gradient Boosting can take a long time on large datasets.\n","\n","* Solution: Use parallel computing, GPU acceleration, or reduce the number of models.\n","\n","**2. More Complexity & Interpretability Issues**\n","  * Difficult to interpret why an ensemble made a certain prediction.\n","  * Individual Decision Trees are easy to explain, but Random Forest is black-box.\n","  * Boosting models are even harder to understand.\n","\n","* Solution: Use feature importance scores and SHAP to interpret predictions.\n","\n","**3. Risk of Overfitting in Boosting Models**\n","* While Bagging reduces overfitting, Boosting can sometimes overfit noisy data.\n","* If too many weak learners are added, the model may fit training data too closely.\n","\n","* Solution: Use early stopping and regularization.\n","\n","**4. Difficult Hyperparameter Tuning**\n","* Ensembles introduce many hyperparameters, making them harder to optimize.\n","  * Example:\n","    * Random Forest: n_estimators, max_depth, max_features\n","    * XGBoost: learning_rate, subsample, colsample_bytree, gamma\n","\n","* Solution: Use Grid Search, Random Search, or Bayesian Optimization for tuning.\n","\n","**5. Increased Memory Usage & Storage Requirements**\n","* Ensembles store multiple models, increasing RAM usage and disk space.\n","  * Example: A single Decision Tree might use 10MB, but a Random Forest with 100 trees could use 1GB+.\n","\n","* Solution: Prune unnecessary models, reduce tree depth, or use model compression techniques."],"metadata":{"id":"diVcKxS0DggX"}},{"cell_type":"markdown","source":["##Q 11. Explain the key idea behind ensemble techniques?\n","**Ans** - The core idea of ensemble techniques is to combine multiple models to improve performance, reduce errors, and increase generalization. Instead of relying on a single model, ensembles aggregate the predictions of multiple models to create a stronger and more reliable predictor.\n","\n","**Use of Ensemble Techniques**\n","\n","\"The wisdom of the crowd\"\n","* Just like multiple opinions lead to better decisions, combining multiple models results in a more robust and accurate prediction.\n","* Individual models may make errors, but combining them reduces their impact.\n","\n","**Working of Ensemble Learning**\n","\n","Ensemble methods work by training multiple weak learners and combining their outputs. The key strategies include:\n","1. Bagging\n","  * Trains multiple models on random subsets of data.\n","  * Final prediction = Majority voting or Averaging.\n","  * Reduces variance & overfitting.\n","  * Best for high-variance models like Decision Trees.\n","2. Boosting\n","  * Models are trained sequentially, where each new model focuses on correcting previous errors.\n","  * Final prediction = Weighted sum of all models.\n","  * Reduces bias & improves accuracy.\n","  * Best for reducing bias in weak models.\n","3. Stacking\n","  * Trains multiple models and combines their outputs using a meta-model.\n","  * The meta-model learns how to optimally combine the base models' predictions.\n","  * More flexible but computationally expensive.\n","4. Voting\n","  * Uses different models and selects the final prediction based on majority voting or averaging.\n","  * Simple but effective.\n","\n","**Advantages of Ensemble Learning**\n","* Higher accuracy than individual models.\n","* Reduces overfitting by averaging out noise.\n","* More stable and robust predictions.\n","* Works well for both bias and variance."],"metadata":{"id":"h_2sOFPRDg3O"}},{"cell_type":"markdown","source":["##Q 12. What is a Random Forest Classifier?\n","**Ans** - A Random Forest Classifier is an ensemble learning algorithm that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting. It is based on Bagging, where each tree is trained on a random subset of data.\n","\n","**Working of Random Forest Classifier**\n","\n","Step-by-Step Process:\n","1. Bootstrap Sampling\n","* The training data is randomly sampled with replacement to create multiple subsets.\n","* Each subset is slightly different but retains the overall distribution.\n","2. Train Multiple Decision Trees\n","* A Decision Tree is trained on each subset.\n","* Trees are independent and trained in parallel.\n","* At each split, only a random subset of features is considered.\n","3. Make Predictions\n","* Each tree makes a separate prediction for a test sample.\n","* The majority vote determines the final classification.\n","\n","      Prediction = Mode(ŷ1,ŷ2,...,ŷN)\n","where N is the number of trees.\n","\n","**Use of Random Forest Classifier**\n","* Higher Accuracy - Reduces variance compared to a single Decision Tree.\n","* Handles Overfitting - Individual trees may overfit, but averaging their predictions prevents it.\n","* Robust to Noise & Missing Data - Works well with large and unclean datasets.\n","* Feature Importance - Identifies important features in classification.\n","* Parallelizable - Can be trained efficiently using multiple processors.\n","\n","**Random Forest Classifier in Python**\n","Using Scikit-Learn:"],"metadata":{"id":"0zGJOdB-DhOu"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X_train, y_train)\n","\n","y_pred = rf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Random Forest Accuracy:\", accuracy)"],"metadata":{"id":"dlJk20-Wx3FV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Parameters of RandomForestClassifier**\n","\n","|Parameter\t|Description|\n","|-||\n","|n_estimators\t|Number of decision trees in the forest|\n","|max_depth\t|Maximum depth of each tree|\n","|max_features\t|Number of features considered at each split|\n","|bootstrap\t|Whether to use bootstrap sampling (default = True)|\n","|oob_score\t|Use Out-of-Bag samples for evaluation|\n","|random_state\t|Controls randomness for reproducibility|\n","\n","**Comparison: Random Forest vs. Decision Tree**\n","\n","|Feature\t|Decision Tree\t|Random Forest|\n","|-|||\n","|Overfitting\t|High (Prone to overfitting)\t|Low (Averages multiple trees)|\n","|Accuracy\t|Moderate\t|High|\n","|Computational Cost\t|Low\t|Higher (Multiple trees)|\n","|Interpretability\t|Easy\t|Harder (Many trees)|"],"metadata":{"id":"IKAi8kHDx7Ik"}},{"cell_type":"markdown","source":["##Q 13. What are the main types of ensemble techniques?\n","**Ans** - Ensemble techniques combine multiple models to improve accuracy, reduce overfitting, and enhance robustness. The three main types of ensemble methods are:\n","\n","**1. Bagging**\n","* Key Idea: Train multiple models on random subsets of data and combine their predictions.\n","* Goal: Reduce variance by averaging multiple models.\n","* Works Best When: The base model has high variance (e.g., Decision Trees).\n","\n","**Working**\n","1. Create random subsets of the training data.\n","2. Train a weak learner (e.g., Decision Tree) on each subset.\n","3. Aggregate predictions:\n","  * Classification: Majority voting\n","  * Regression: Averaging\n","\n","**Example Algorithms:**\n","* Random Forest\n","* Bagging Classifier\n","* Bagging Regressor\n","\n","**2. Boosting**\n","* Key Idea: Train models sequentially, where each model focuses on correcting errors of the previous one.\n","* Goal: Reduce bias and improve predictive power.\n","* Works Best When: The base model has high bias (e.g., shallow Decision Trees).\n","\n","**Working**\n","1. Train a weak learner on the data.\n","2. Identify misclassified samples and assign them higher weights.\n","3. Train the next model on the updated data.\n","4. Final prediction is a weighted sum of all models.\n","\n","**Example Algorithms:**\n","* AdaBoost\n","* Gradient Boosting\n","* XGBoost\n","* LightGBM\n","* CatBoost\n","\n","**3. Stacking**\n","* Key Idea: Combine multiple models by training a meta-model that learns how to best combine their outputs.\n","* Goal: Improve predictive performance by leveraging the strengths of different models.\n","* Works Best When: Base models have diverse strengths.\n","\n","**Working**\n","1. Train multiple base models (e.g., Random Forest, SVM, Neural Network).\n","2. Collect predictions from each model.\n","3. Train a meta-model (e.g., Linear Regression, another ML model) to combine these predictions optimally.\n","\n","**Example Algorithm:**\n","* StackingClassifier\n","\n","**Comparison of Ensemble Techniques**\n","\n","|Technique\t|Goal\t|Reduces Bias or Variance?\t|Example Algorithm|\n","|-||||\n","|Bagging\t|Reduce Overfitting\t|Reduces Variance\t|Random Forest|\n","|Boosting\t|Improve Accuracy\t|Reduces Bias\t|XGBoost|\n","|Stacking\t|Combine Diverse Models\t|Both Bias & Variance\t|Stacking Classifier|"],"metadata":{"id":"HP-wHEUzDhnW"}},{"cell_type":"markdown","source":["##Q 14. What is ensemble learning in machine learning?\n","**Ans** - Ensemble Learning is a technique in machine learning where multiple models are combined to improve accuracy, stability, and generalization compared to a single model.\n","\n","Instead of relying on a single model, ensemble methods aggregate multiple predictions to reduce errors and make more reliable decisions.\n","\n","**Use of Ensemble Learning**\n","1. Higher Accuracy - Combines weak models to create a stronger model.\n","2. Reduces Overfitting - Averaging multiple models prevents over-reliance on one.\n","3. More Stable & Robust - Less affected by noise in the dataset.\n","4. Works with Any ML Algorithm - Can combine Decision Trees, Neural Networks, SVMs, etc.\n","\n","**Types of Ensemble Learning Techniques**\n","\n","Ensemble learning methods fall into three major categories:\n","\n","**1. Bagging**\n","  * Key Idea: Train multiple models on random subsets of data and combine predictions.\n","  * Goal: Reduce variance.\n","  * Works Best For: High-variance models like Decision Trees.\n","\n","**Example Algorithms:**\n","  * Random Forest\n","  * Bagging Classifier\n","\n","**Bagging working**\n","1. Randomly sample data with replacement.\n","2. Train a separate model on each subset.\n","3. Final prediction: Majority voting or averaging.\n","\n","**2. Boosting**\n","* Key Idea: Train models sequentially, where each model corrects errors of the previous one.\n","* Goal: Reduce bias.\n","* Works Best For: Weak models like shallow Decision Trees.\n","\n","**Example Algorithms:**\n","* AdaBoost\n","* Gradient Boosting\n","* XGBoost\n","* LightGBM\n","\n","**Boosting Working**\n","1. Train a weak model.\n","2. Identify misclassified samples and assign them higher weights.\n","3. Train the next model with updated weights.\n","4. Final prediction: Weighted sum of all models.\n","\n","**3. Stacking**\n","* Key Idea: Combine multiple different models and use a meta-model to learn the best combination.\n","* Goal: Improve predictive performance by leveraging different models' strengths.\n","* Works Best For: Diverse model types with complementary strengths.\n","\n","**Example Algorithm:**\n","* StackingClassifier\n","\n","**Stacking Working**\n","1. Train multiple base models.\n","2. Collect predictions from each base model.\n","3. Train a meta-model to combine their outputs optimally.\n","\n","**Comparison of Ensemble Methods**\n","\n","|Technique\t|Goal\t|Reduces Bias or Variance?\t|Example Algorithm|\n","|-||||\n","|Bagging\t|Reduce Overfitting\t|Reduces Variance\t|Random Forest|\n","|Boosting\t|Improve Accuracy\t|Reduces Bias\t|XGBoost|\n","|Stacking\t|Combine Diverse Models\t|Both Bias & Variance\t|Stacking Classifier|\n","\n","**Example: Random Forest in Python**"],"metadata":{"id":"8pfxdRjWDiBP"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X_train, y_train)\n","\n","y_pred = rf.predict(X_test)\n","\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))"],"metadata":{"id":"WxxuYWIL1nGq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Q 15. When should we avoid using ensemble methods?\n","**Ans** - Ensemble learning is powerful, but it isn't always the best choice. Here are cases when you should avoid using ensemble methods:\n","\n","**1. When we Need Interpretability**\n","* Problem: Ensembles are black-box models, making it difficult to explain individual predictions.\n","* Better Alternative: Use simpler models like Decision Trees, Logistic Regression, or Linear Regression if interpretability is crucial.\n","\n","**2. When we have limited Computational Resources**\n","* Problem: Ensembles require more memory, computation, and training time than single models.\n","* Example:\n","  * A single Decision Tree trains in seconds, while a Random Forest takes much longer.\n","  * Boosting requires multiple sequential training rounds, increasing processing time.\n","* Better Alternative: Use a single efficient model.\n","\n","**3. When a Single Model Performs Well**\n","* Problem: If a single model already achieves high accuracy, adding an ensemble may provide minimal improvements while increasing complexity.\n","* Better Alternative: Instead of using ensembles, try hyperparameter tuning or feature engineering to improve a single model.\n","\n","**4. When we have a Small Dataset**\n","* Problem: Ensembles require a lot of data to generalize well.\n","* If the dataset is small, ensemble models might overfit instead of improving performance.\n","* Better Alternative: Use simpler models like Logistic Regression, SVM, or a single Decision Tree, which can perform well on small datasets.\n","\n","**5. When Real-Time Predictions Are Needed**\n","* Problem: Ensembles, especially Stacking and Boosting, can be slow during inference.\n","* Example: A single Neural Network can predict in milliseconds, while a Stacked model with multiple base learners can take seconds.\n","* Better Alternative: Use a lightweight single model (e.g., a pruned Decision Tree or a small Neural Network).\n","\n","**6. When Ensemble Diversity Is Low**\n","* Problem: If the base models are too similar, ensemble learning won't add much benefit.\n","* Example: Training multiple Logistic Regression models won't work well because they all behave similarly.\n","* Better Alternative: Ensure base models are diverse or just use the best single model.\n","\n","**When to Avoid Ensembles**\n","\n","|Avoid Ensembles When... |Better Alternative|\n","|-||\n","|You need explainable predictions\t|Decision Tree, Logistic Regression|\n","|You have limited computing power\t|A single, well-tuned model|\n","|A single model already performs well\t|Hyperparameter tuning|\n","|You have a small dataset\t|Simple models (SVM, Decision Tree)|\n","|You need real-time predictions\t|Lightweight models (Neural Networks, Pruned Trees)|\n","|The models in the ensemble are too similar\t|Choose the best individual model|"],"metadata":{"id":"E39U_DxaDiZO"}},{"cell_type":"markdown","source":["##Q 16. How does Bagging help in reducing overfitting?\n","**Ans** - Bagging reduces overfitting by combining multiple weak learners and averaging their predictions, making the final model more stable, robust, and less sensitive to noise.\n","\n","**Reasons Why Bagging Reduces Overfitting**\n","**1. Reduces Variance**\n","* Overfitting occurs when a model learns too much from noise in the training data.\n","* Decision Trees are prone to overfitting.\n","* Bagging reduces variance by averaging predictions from multiple models, preventing any single model from dominating.\n","* Analogy: Like taking multiple opinions to make a fair decision rather than relying on just one person's judgment.\n","  * Example: Instead of relying on one overfitted Decision Tree, Bagging trains multiple slightly different trees and averages their predictions.\n","\n","**2. Uses Bootstrap Sampling**\n","* Each model in Bagging is trained on a random subset of data.\n","* This introduces diversity, preventing models from memorizing noise in the data.\n","* As a result, overfitting to a specific training set is minimized.\n","  * Example:\n","    * A single Decision Tree sees the entire dataset and overfits.\n","    * In Bagging, each tree sees only a portion of the data, reducing the risk of overfitting.\n","\n","**3. Reduces Model Dependence**\n","* If a single model overfits, its prediction is biased.\n","* Bagging makes models independent by training them on different datasets.\n","* When multiple models work together, errors of individual models cancel out.\n","  * Example:\n","    * If one tree incorrectly predicts a class, other trees may correct it, leading to a more balanced prediction.\n","\n","**4. Works Well with High-Variance Models**\n","* Bagging is especially useful for models like Decision Trees, which tend to overfit due to their deep structures.\n","* By averaging multiple overfitted models, Bagging smooths out extreme predictions.\n","  * Example:\n","    * A single deep Decision Tree memorizes patterns.\n","    * A Random Forest generalizes better by averaging multiple trees.\n","\n","**Example: Bagging with Random Forest in Python**"],"metadata":{"id":"HdPUaRCWDixm"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X_train, y_train)\n","\n","y_pred = rf.predict(X_test)\n","\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))"],"metadata":{"id":"S_UiU62F_tph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary**\n","\n","|Bagging Technique\t|How It Reduces Overfitting?|\n","|-||\n","|Reduces Variance\t|Combines multiple models to smooth out extreme predictions|\n","|Bootstrap Sampling\t|Trains each model on different subsets to prevent memorization|\n","|Independent Models\t|Reduces dependence on any single overfitted model|\n","|Averaging Predictions\t|Smooths out fluctuations and noise|"],"metadata":{"id":"Kh4-7F9K_z2d"}},{"cell_type":"markdown","source":["##Q 17. Why is Random Forest better than a single Decision Tree?\n","**Ans** - A Random Forest is an ensemble method that combines multiple Decision Trees, making it more accurate, robust, and less prone to overfitting than a single Decision Tree.\n","\n","**Advantages of Random Forest Over a Decision Tree**\n","1. Reduces Overfitting\n","* Decision Trees tend to overfit, especially deep trees.\n","* Random Forest reduces overfitting by averaging multiple trees, preventing any single tree from dominating.\n","* Result: More generalized model, better performance on new data.\n","* Example:\n","    A single Decision Tree might memorize noise and give 98% accuracy on training data but only 80% on test data.\n","    A Random Forest might have 94% accuracy on training data and 90% on test data—better generalization!\n","\n","2. More Stable and Robust\n","* A single Decision Tree can be unstable: small changes in data can drastically change its structure.\n","* Random Forest averages results from multiple trees, making it more consistent and stable.\n","* Result: More reliable predictions.\n","  * Example:\n","    If you remove a few data points, a Decision Tree might change its structure entirely, but Random Forest remains stable.\n","\n","**3. Handles Missing and Noisy Data Better**\n","* Decision Trees are sensitive to outliers and missing values.\n","* Random Forest handles these better because:\n","  * It trains on different random subsets of data.\n","  * The impact of a few bad trees is minimized by averaging multiple trees.\n","* Example:\n","  If your dataset has some wrong labels or missing values, Random Forest smooths out their effect, while a single Decision Tree might be misled.\n","\n","**4. Works Well for High-Dimensional Data**\n","* Decision Trees consider all features at each split, which can lead to overfitting in high-dimensional data.\n","* Random Forest uses Feature Randomness, making it more efficient.\n","* Result: Avoids bias from dominant features, leading to better performance.\n","\n","* Example:\n","  If we have 100 features, Random Forest doesn't use all 100 in every tree. Each tree picks, say, 10 random features, leading to more diverse trees and better predictions.\n","\n","**5. More Accurate Predictions**\n","* A single Decision Tree gives just one prediction, which might be incorrect if the tree overfits.\n","* Random Forest takes a majority vote or averages predictions from multiple trees, leading to more accurate results.\n","\n","* Example:\n","  If one tree misclassifies a data point, but 7 out of 10 trees classify it correctly, Random Forest will likely predict correctly.\n","\n","**6. Reduces the Impact of Outliers**\n","* Decision Trees can be skewed by extreme outliers.\n","* Random Forest minimizes this effect by averaging results across multiple trees.\n","* Result: More robust predictions.\n","\n","* Example:\n","  If one tree is misled by an extreme outlier, other trees in the forest help balance the final decision.\n","\n","**Example: Random Forest vs. Decision Tree in Python**"],"metadata":{"id":"Zl05TgRPDjHu"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","dt = DecisionTreeClassifier(random_state=42)\n","dt.fit(X_train, y_train)\n","dt_pred = dt.predict(X_test)\n","\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X_train, y_train)\n","rf_pred = rf.predict(X_test)\n","\n","print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))"],"metadata":{"id":"rnu2idRSB6rw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Expected Result: Random Forest will have higher accuracy than the Decision Tree.\n","\n","**Random Forest Better**\n","\n","|Feature\t|Decision Tree\t|Random Forest|\n","|-|||\n","|Overfitting?\t|High Risk\t|Low Risk|\n","|Stability?\t|Unstable\t|Stable|\n","|Handles Noisy Data?\t|Poorly |Well|\n","|Feature Selection?\t|Uses all features\t|Uses random subsets|\n","|Computationally Expensive?\t|Fast\t|Slower|\n","|Prediction Accuracy?\t|Lower |Higher|\n","|Sensitive to Outliers?\t|Yes | No|"],"metadata":{"id":"sdA7loEzCARy"}},{"cell_type":"markdown","source":["##Q 18. What is the role of bootstrap sampling in Bagging?\n","**Ans** - Bootstrap Sampling is a crucial technique used in Bagging to create multiple diverse training datasets from the original data. This helps improve stability, accuracy, and generalization while reducing overfitting.\n","\n","**Bootstrap Sampling**\n","* Definition: It's a random sampling method with replacement where each dataset is generated by selecting random instances from the original dataset multiple times.\n","* Key Idea: Some data points appear multiple times, while others might not appear at all in each sampled dataset.\n","* Size: Each bootstrap sample is equal in size to the original dataset.\n","\n","* Example:\n","  If we have 10,000 data points, each bootstrap sample will also have 10,000 points, but some may repeat while others are missing.\n","\n","**Bootstrap Sampling is Important in Bagging**\n","\n","**1. Creates Diversity**\n","* Since each model is trained on a different subset of data, it prevents overfitting to any single dataset.\n","* When combined, errors from individual models are canceled out, leading to better generalization.\n","\n","* Example:\n","  If one dataset has noisy data, another dataset may exclude it, balancing the final prediction.\n","\n","**2. Reduces Variance**\n","* Decision Trees and other high-variance models tend to overfit the training data.\n","* By averaging multiple models trained on different bootstrap samples, variance is reduced, making predictions more stable and reliable.\n","\n","* Example:\n","  A single Decision Tree might give very different results on slightly different datasets, while a Bagged ensemble of trees produces consistent results.\n","\n","**3. Increases Model Stability**\n","* A single model trained on the entire dataset might be highly sensitive to noise.\n","* Bagging trains multiple models on different subsets, reducing the impact of noisy samples on overall predictions.\n","\n","* Example:\n","  If a few noisy data points mislead one model, other models compensate, leading to better overall accuracy.\n","\n","**4. Enables Out-of-Bag Error Estimation**\n","* Some data points are left out in each bootstrap sample.\n","* These unused data points can be used to evaluate the model's performance without needing a separate validation set.\n","* This saves data and prevents overfitting to a validation set.\n","\n","* Example:\n","  In Random Forest, OOB score is used as a built-in validation metric without needing cross-validation.\n","\n","**Example: Bootstrap Sampling in Python**"],"metadata":{"id":"q5h2RG9pDjeo"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.utils import resample\n","\n","data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n","\n","bootstrap_sample = resample(data, replace=True, n_samples=len(data), random_state=42)\n","\n","print(\"Original Data:\", data)\n","print(\"Bootstrap Sample:\", bootstrap_sample)"],"metadata":{"id":"Ky_9vWc0KQbW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Output Example:"],"metadata":{"id":"wbYEWxM3KV_n"}},{"cell_type":"code","source":["Original Data: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","Bootstrap Sample: [6, 1, 8, 3, 10, 2, 2, 6, 4, 10]"],"metadata":{"id":"lYrmL8NQKaht"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Shows how some values repeat while others are missing, ensuring diversity in training models.\n","\n","**Bootstrap Sampling in Bagging**\n","\n","|Bootstrap Sampling Helps By...\t|Impact on Bagging|\n","|-||\n","|Creating Diverse Training Sets\t|Reduces overfitting|\n","|Reducing Model Variance\t|Improves generalization|\n","|Handling Noisy Data\t|Increases robustness|\n","|Enabling OOB Error Estimation\t|Eliminates need for extra validation data|"],"metadata":{"id":"492b88u-KdkN"}},{"cell_type":"markdown","source":["##Q 19. What are some real-world applications of ensemble techniques?\n","**Ans** - Ensemble techniques are widely used in various fields to improve accuracy, robustness, and generalization. Here are some key real-world applications:\n","\n","**1. Fraud Detection**\n","* Problem: Fraudulent transactions are rare but crucial to detect.\n","* Solution: Random Forest, XGBoost, and Stacking combine multiple models to detect fraud more accurately than a single model.\n","* Why Ensemble\n","  * Reduces false positives.\n","  * Captures complex fraud patterns using multiple models.\n","\n","* Example:\n","  * Credit card fraud detection.\n","  * Loan default prediction.\n","\n","**2. Healthcare & Medical Diagnosis**\n","* Problem: Diagnosing diseases from medical images, reports, or genetic data requires high precision.\n","* Solution: Bagging, Boosting, and Deep Learning ensembles improve diagnostic accuracy.\n","* Why Ensemble\n","  * Reduces misclassification of critical diseases.\n","  * Combines different models to handle complex medical data.\n","\n","* Example:\n","  * Cancer detection using ensemble models from MRI scans.\n","  * Heart disease prediction based on multiple patient parameters.\n","  * COVID-19 diagnosis using ensemble learning on CT scans and X-rays.\n","\n","**3. Stock Market Prediction**\n","* Problem: Stock prices are influenced by multiple factors, making prediction difficult.\n","* Solution: Stacking improves forecasting.\n","* Why Ensemble\n","  * Reduces risk by combining multiple predictive models.\n","  * Captures different patterns.\n","\n","* Example:\n","  * Portfolio risk assessment using Random Forest and Gradient Boosting.\n","  * Stock price trend prediction using ensemble models.\n","\n","**4. Customer Churn Prediction**\n","* Problem: Companies need to predict which customers will leave.\n","* Solution: Boosting and Bagging models help identify high-risk customers.\n","* Why Ensemble\n","  * Detects subtle patterns leading to customer churn.\n","  * Improves marketing strategies to retain customers.\n","\n","* Example:\n","  * Netflix churn prediction.\n","  * Amazon customer retention.\n","  * Telecom industry churn prediction.\n","\n","**5. Recommendation Systems**\n","* Problem: Users expect personalized recommendations.\n","* Solution: Ensemble learning improves recommendation accuracy.\n","\n","* Why Ensemble\n","  * Combines content-based and collaborative filtering.\n","  * Learns user preferences more effectively.\n","\n","* Example:\n","  * Netflix movie recommendations.\n","  * Amazon product recommendations.\n","  * Spotify personalized playlists.\n","\n","**6. Image & Object Recognition**\n","* Problem: Recognizing objects in images requires high accuracy.\n","* Solution: Ensemble models improve classification.\n","* Why Ensemble\n","  * Works well in noisy images.\n","  * Combines multiple models to detect objects more accurately.\n","\n","* Example:\n","  * Self-driving cars - detecting pedestrians, road signs.\n","  * Facial recognition.\n","  * Medical imaging.\n","\n","**7. NLP: Sentiment Analysis & Chatbots**\n","* Problem: Understanding human emotions in text is challenging.\n","* Solution: Stacking enhances NLP accuracy.\n","* Why Ensemble\n","  * Captures different aspects of sentiment.\n","  * Improves chatbot responses using multi-model learning.\n","\n","* Example:\n","  * Twitter sentiment analysis for brand reputation monitoring.\n","  * Customer support chatbots.\n","\n","**8. Cybersecurity & Intrusion Detection**\n","* Problem: Identifying cyber threats in real-time.\n","* Solution: Random Forest, XGBoost, and Stacking ensembles detect anomalies in network traffic.\n","* Why Ensemble\n","  * Detects both known and unknown threats.\n","  * Reduces false alarms while catching real attacks.\n","\n","* Example:\n","  * Spam detection.\n","  * Malware detection using ensemble models in antivirus software.\n","\n","**Real-World Use Cases of Ensemble Learning**\n","\n","|Application\t|Industry\t|Ensemble Techniques Used|\n","|-|||\n","|Fraud Detection\t|Banking, Fintech\t|Random Forest, XGBoost, Stacking|\n","|Medical Diagnosis\t|Healthcare\t|CNN ensembles, Boosting|\n","|Stock Market Prediction\t|Finance\t|Stacking, Gradient Boosting|\n","|Customer Churn Prediction\t|Telecom, E-commerce\t|Random Forest, XGBoost|\n","|Recommendation Systems\t|Retail, Streaming\t|Hybrid Ensembles|\n","|Object Recognition\t|Autonomous Vehicles\t|CNN Ensembles|\n","|Sentiment Analysis\t|Social Media, NLP\t|Stacking, Transformers|\n","|Cybersecurity\t|IT Security\t|Random Forest, Isolation Forest|"],"metadata":{"id":"dxnGpIVfDj1W"}},{"cell_type":"markdown","source":["##Q 20. What is the difference between Bagging and Boosting?\n","**Ans** - Bagging and Boosting are two powerful ensemble learning techniques used to improve machine learning model performance. However, they differ in how they train models and combine predictions.\n","\n","**1. Bagging**\n","* Goal: Reduce variance.\n","* it's Workimg:\n","  * Creates multiple subsets of the original data using bootstrap sampling.\n","  * Trains multiple independent models on these subsets in parallel.\n","  * Combines predictions using majority voting or averaging.\n","\n","* Example: Random Forest\n","* A Random Forest is an ensemble of Decision Trees built using Bagging.\n","* Each tree is trained on a different subset of data, and the final prediction is based on the majority vote or average prediction.\n","\n","**Advantages of Bagging**\n","* Reduces overfitting.\n","* Works well when individual models have high variance but low bias.\n","* Improves stability and accuracy.\n","\n","**Disadvantages of Bagging**\n","* Does not perform well with models that already have low variance.\n","* Requires more computation due to multiple models running in parallel.\n","\n","**2. Boosting**\n","* Goal: Reduce bias.\n","* Working:\n","  * Trains models sequentially, where each new model focuses on correcting the mistakes of the previous model.\n","  * Assigns higher weights to misclassified samples, forcing the model to focus on hard-to-classify data points.\n","  * Final prediction is a weighted sum of all weak learners.\n","\n","**Example: AdaBoost, Gradient Boosting, XGBoost**\n","* AdaBoost: Adjusts sample weights after each iteration.\n","* Gradient Boosting: Uses gradient descent to minimize errors.\n","* XGBoost is an optimized version of Gradient Boosting, used widely in Kaggle competitions and real-world applications.\n","\n","**Advantages of Boosting**\n","* Reduces bias.\n","* Works well with small datasets with high bias.\n","* Produces highly accurate models.\n","\n","**Disadvantages of Boosting**\n","* Slower training due to sequential learning.\n","* Sensitive to noisy data.\n","* Harder to parallelize compared to Bagging.\n","\n","**Differences Between Bagging & Boosting**\n","\n","|Feature\t|Bagging\t|Boosting|\n","|-|||\n","|Goal\t|Reduce variance (overfitting)\t|Reduce bias (improve weak models)|\n","|Training\t|Parallel (independent models)\t|Sequential (each model corrects previous errors)|\n","|Data Sampling\t|Bootstrap Sampling (random with replacement)\t|Weighted Sampling (focuses on misclassified data)|\n","|Model Combination\t|Majority voting (classification) / Averaging (regression)\t|Weighted sum of models|\n","|Overfitting Risk\t|Less overfitting\t|Higher risk of overfitting if not regularized|\n","|Best For\t|High-variance models (e.g., Decision Trees)\t|High-bias models (e.g., Weak learners like Decision Stumps)|\n","|Popular Algorithms\t|Random Forest\t|AdaBoost, Gradient Boosting, XGBoost, LightGBM|\n","|Training Speed\t|Faster (independent models train in parallel)\t|Slower (models train sequentially)|\n","\n","**Use of Bagging vs. Boosting**\n","* Use Bagging if:\n","  * The base model has high variance.\n","  * We want a stable, robust model with lower overfitting risk.\n","  * We need parallel training for faster computation.\n","\n","* Use Boosting if:\n","  * The base model has high bias.\n","  * We need higher accuracy and are willing to fine-tune parameters.\n","  * We have a small dataset with complex patterns."],"metadata":{"id":"OTc8jO6UDkLJ"}},{"cell_type":"markdown","source":["#Practical"],"metadata":{"id":"JABw6ZZuEPFF"}},{"cell_type":"markdown","source":["##Q 21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n","**Ans** - Python implementation of a Bagging Classifier using Decision Trees on a sample dataset (Iris dataset). We will train the model and print its accuracy."],"metadata":{"id":"UCkAUjYEDkgW"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","bagging_clf = BaggingClassifier(\n","    base_estimator=DecisionTreeClassifier(),\n","    n_estimators=10,\n","    random_state=42\n",")\n","\n","bagging_clf.fit(X_train, y_train)\n","\n","y_pred = bagging_clf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Bagging Classifier Accuracy: {accuracy:.2f}\")"],"metadata":{"id":"BqI8D9A6bbEh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Iris dataset.\n","* Bagging Classifier: Uses Decision Trees as the base model.\n","* Model Training: Trains 10 trees in parallel using different bootstrap samples.\n","* Evaluation: Computes accuracy on the test set."],"metadata":{"id":"ftkuvmyAbe1w"}},{"cell_type":"markdown","source":["##Q 22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n","**Ans** - Python implementation of a Bagging Regressor using Decision Trees, evaluated with Mean Squared Error (MSE) on the Boston Housing dataset."],"metadata":{"id":"hePdEUVIDk2W"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","data = fetch_california_housing()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","bagging_reg = BaggingRegressor(\n","    base_estimator=DecisionTreeRegressor(),\n","    n_estimators=10,\n","    random_state=42\n",")\n","\n","bagging_reg.fit(X_train, y_train)\n","\n","y_pred = bagging_reg.predict(X_test)\n","\n","mse = mean_squared_error(y_test, y_pred)\n","\n","print(f\"Bagging Regressor Mean Squared Error: {mse:.2f}\")"],"metadata":{"id":"44t0_vjBbu-x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the California Housing datasets.\n","* Bagging Regressor: Uses Decision Trees as base learners.\n","* Model Training: Trains 10 Decision Trees in parallel on different bootstrap samples.\n","* Evaluation: Uses Mean Squared Error to measure model performance."],"metadata":{"id":"MApD89DDbyXb"}},{"cell_type":"markdown","source":["##Q 23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n","**Ans** - Python implementation to train a Random Forest Classifier on the Breast Cancer dataset and print the feature importance scores."],"metadata":{"id":"Csm8JI4hDlMK"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","feature_names = data.feature_names\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","\n","rf_clf.fit(X_train, y_train)\n","\n","y_pred = rf_clf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","feature_importance = rf_clf.feature_importances_\n","\n","feature_importance_df = pd.DataFrame({\n","    'Feature': feature_names,\n","    'Importance Score': feature_importance\n","}).sort_values(by='Importance Score', ascending=False)\n","\n","print(f\"Random Forest Classifier Accuracy: {accuracy:.2f}\\n\")\n","print(\"Feature Importance Scores:\\n\")\n","print(feature_importance_df)"],"metadata":{"id":"L_5LO0AqcCkJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset (binary classification problem).\n","* Random Forest Classifier: Trains 100 Decision Trees.\n","* Model Training: Trains on 80% of the data and tests on 20%.\n","* Evaluation: Measures accuracy of the model.\n","* Feature Importance: Extracts and prints the most important features for classification."],"metadata":{"id":"kSOIqOu_cGPK"}},{"cell_type":"markdown","source":["##Q 24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n","**Ans** - Python implementation to train both a Random Forest Regressor and a single Decision Tree Regressor, then compare their performance using Mean Squared Error."],"metadata":{"id":"YOOGfDcHDliX"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","data = fetch_california_housing()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","dt_regressor = DecisionTreeRegressor(random_state=42)\n","dt_regressor.fit(X_train, y_train)\n","y_pred_dt = dt_regressor.predict(X_test)\n","mse_dt = mean_squared_error(y_test, y_pred_dt)\n","\n","rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf_regressor.fit(X_train, y_train)\n","y_pred_rf = rf_regressor.predict(X_test)\n","mse_rf = mean_squared_error(y_test, y_pred_rf)\n","\n","print(f\"Decision Tree Regressor MSE: {mse_dt:.2f}\")\n","print(f\"Random Forest Regressor MSE: {mse_rf:.2f}\")\n","\n","if mse_rf < mse_dt:\n","    print(\"Random Forest performs better than a single Decision Tree!\")\n","else:\n","    print(\"Decision Tree performs better, which is unusual! Consider tuning hyperparameters.\")"],"metadata":{"id":"VbFIIxupcWCJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the California Housing dataset.\n","* Decision Tree Regressor: Trains a single tree, which may overfit.\n","* Random Forest Regressor: Trains 100 trees and averages their predictions to reduce overfitting.\n","* Performance Metric: Compares Mean Squared Error of both models.\n","\n","* Expected Outcome:\n","  * Random Forest typically has a lower MSE because it reduces variance by averaging multiple trees.\n","  * A single Decision Tree may overfit, leading to a higher MSE."],"metadata":{"id":"TInsQlmtcbyR"}},{"cell_type":"markdown","source":["##Q 25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n","**Ans** - Python implementation to train a Random Forest Classifier and compute the Out-of-Bag Score on the Breast Cancer dataset."],"metadata":{"id":"o_fZngbEDl7u"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42, bootstrap=True)\n","\n","rf_clf.fit(X_train, y_train)\n","\n","oob_score = rf_clf.oob_score_\n","\n","y_pred = rf_clf.predict(X_test)\n","test_accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Out-of-Bag (OOB) Score: {oob_score:.2f}\")\n","print(f\"Test Accuracy: {test_accuracy:.2f}\")"],"metadata":{"id":"cqdOQkIBcuRB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation**\n","* Dataset: Uses the Breast Cancer dataset.\n","* OOB Score: Enabled by setting oob_score=True. This estimates model performance using out-of-bag samples.\n","* Model Training: Trains a Random Forest Classifier with 100 trees.\n","* Evaluation: Compares OOB Score and test set accuracy.\n","\n","**Expected Outcome:**\n","* The OOB Score should be close to the test accuracy, showing it is a good estimate of model performance.\n","* OOB validation is useful when cross-validation is expensive or unnecessary."],"metadata":{"id":"rDC-px3rcyvB"}},{"cell_type":"markdown","source":["##Q 26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n","**Ans** - Python implementation to train a Bagging Classifier using SVM as the base estimator and print its accuracy on the Iris dataset."],"metadata":{"id":"5zHcBp1WDmoX"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.svm import SVC\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","bagging_clf = BaggingClassifier(\n","    base_estimator=SVC(),\n","    n_estimators=10,\n","    random_state=42\n",")\n","\n","bagging_clf.fit(X_train, y_train)\n","\n","y_pred = bagging_clf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Bagging Classifier (SVM) Accuracy: {accuracy:.2f}\")"],"metadata":{"id":"O8Jq_hVydI3p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Iris dataset.\n","* Bagging Classifier: Uses SVM as the base estimator.\n","* Bootstrap Sampling: Each base SVM is trained on different subsets of the data.\n","* Model Training: Trains 10 SVM models in parallel.\n","* Evaluation: Prints test set accuracy.\n","\n","**Expected Outcome**\n","* The Bagging Classifier improves stability and generalization of SVM.\n","* Can help SVM handle high variance cases more effectively."],"metadata":{"id":"1P-tHUg0dNep"}},{"cell_type":"markdown","source":["##Q 27. Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n","**Ans** - Python implementation to train a Random Forest Classifier with different numbers of trees and compare their accuracy on the Breast Cancer dataset."],"metadata":{"id":"lDfvIqjmDm-3"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","n_estimators_list = [1, 5, 10, 50, 100, 200, 500]\n","accuracies = []\n","\n","for n in n_estimators_list:\n","    rf_clf = RandomForestClassifier(n_estimators=n, random_state=42)\n","    rf_clf.fit(X_train, y_train)\n","\n","    y_pred = rf_clf.predict(X_test)\n","\n","    accuracy = accuracy_score(y_test, y_pred)\n","    accuracies.append(accuracy)\n","\n","    print(f\"Random Forest (n_estimators={n}) Accuracy: {accuracy:.2f}\")\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(n_estimators_list, accuracies, marker='o', linestyle='-')\n","plt.xlabel(\"Number of Trees (n_estimators)\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Random Forest Accuracy vs. Number of Trees\")\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"Lp0UpF2MdgqB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset.\n","* Random Forest Classifier: Trains models with different numbers of trees.\n","* Model Training: Trains models with 1, 5, 10, 50, 100, 200, 500 trees.\n","* Evaluation: Computes test accuracy for each model.\n","* Visualization: Plots accuracy vs. number of trees to show improvement trends.\n","\n","**Expected Outcome:**\n","* More trees usually improve accuracy, but after a certain point, the improvement is marginal.\n","* Plot helps visualize diminishing returns beyond a certain number of trees."],"metadata":{"id":"-ZXb_rrTdkyB"}},{"cell_type":"markdown","source":["##Q 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n","**Ans** - Python implementation to train a Bagging Classifier using Logistic Regression as the base estimator and print the AUC score on the Breast Cancer dataset."],"metadata":{"id":"O90VCsXUDnd_"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","bagging_clf = BaggingClassifier(\n","    base_estimator=LogisticRegression(max_iter=5000),\n","    n_estimators=10,\n","    random_state=42\n",")\n","\n","bagging_clf.fit(X_train, y_train)\n","\n","y_pred_prob = bagging_clf.predict_proba(X_test)[:, 1]\n","\n","auc_score = roc_auc_score(y_test, y_pred_prob)\n","\n","print(f\"Bagging Classifier (Logistic Regression) AUC Score: {auc_score:.2f}\")"],"metadata":{"id":"c6dRkqSPd2uJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset.\n","* Bagging Classifier: Uses Logistic Regression as the base model.\n","* Bootstrap Sampling: Each Logistic Regression model is trained on different random subsets of the data.\n","* Evaluation Metric: Computes AUC score.\n","* Probability Predictions: Uses predict_proba() to get probabilities for AUC calculation.\n","\n","**Expected Outcome:**\n","* AUC Score close to 1 indicates a good classifier.\n","* Bagging helps Logistic Regression by reducing variance."],"metadata":{"id":"mdxA9FQ7d6gg"}},{"cell_type":"markdown","source":["##Q 29. Train a Random Forest Regressor and analyze feature importance scores.\n","**Ans** - Python implementation to train a Random Forest Regressor on the California Housing dataset and analyze feature importance scores."],"metadata":{"id":"3hwkLN7eDn0o"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","data = fetch_california_housing()\n","X, y = data.data, data.target\n","feature_names = data.feature_names\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf_regressor.fit(X_train, y_train)\n","\n","y_pred = rf_regressor.predict(X_test)\n","\n","mse = mean_squared_error(y_test, y_pred)\n","\n","feature_importance = rf_regressor.feature_importances_\n","\n","feature_importance_df = pd.DataFrame({\n","    'Feature': feature_names,\n","    'Importance Score': feature_importance\n","}).sort_values(by='Importance Score', ascending=False)\n","\n","print(f\"Random Forest Regressor MSE: {mse:.2f}\\n\")\n","print(\"Feature Importance Scores:\\n\")\n","print(feature_importance_df)\n","\n","plt.figure(figsize=(8, 5))\n","plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance Score'], color='skyblue')\n","plt.xlabel(\"Feature Importance Score\")\n","plt.ylabel(\"Feature\")\n","plt.title(\"Feature Importance in Random Forest Regressor\")\n","plt.gca().invert_yaxis()\n","plt.show()"],"metadata":{"id":"Lqlf-uJleJ_w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the California Housing dataset.\n","* Random Forest Regressor: Trains 100 trees to predict housing prices.\n","* Evaluation: Computes Mean Squared Error to measure performance.\n","* Feature Importance: Extracts and ranks most important features for predicting house prices.\n","* Visualization: Uses a bar chart to display feature importance scores.\n","\n","**Expected Outcome:**\n","* Lower MSE = Better model performance.\n","* Features like \"MedInc\" and \"AveRooms\" are often the most important.\n","* Plot helps visualize which features contribute the most."],"metadata":{"id":"pIcAZYNZeOWj"}},{"cell_type":"markdown","source":["##Q 30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n","**Ans** - Python implementation to train both a Bagging Classifier and a Random Forest Classifier on the Breast Cancer dataset and compare their accuracy."],"metadata":{"id":"gqzvF_OZDoPf"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","bagging_clf = BaggingClassifier(\n","    base_estimator=DecisionTreeClassifier(),\n","    n_estimators=100,\n","    random_state=42\n",")\n","bagging_clf.fit(X_train, y_train)\n","\n","rf_clf = RandomForestClassifier(\n","    n_estimators=100,\n","    random_state=42\n",")\n","rf_clf.fit(X_train, y_train)\n","\n","y_pred_bagging = bagging_clf.predict(X_test)\n","y_pred_rf = rf_clf.predict(X_test)\n","\n","accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n","accuracy_rf = accuracy_score(y_test, y_pred_rf)\n","\n","print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")\n","print(f\"Random Forest Classifier Accuracy: {accuracy_rf:.2f}\")\n","\n","if accuracy_rf > accuracy_bagging:\n","    print(\"Random Forest performs better!\")\n","elif accuracy_bagging > accuracy_rf:\n","    print(\"Bagging Classifier performs better!\")\n","else:\n","    print(\"Both models have similar accuracy!\")"],"metadata":{"id":"Y75bSq7SemoJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset.\n","* Bagging Classifier: Uses 100 Decision Trees as base learners.\n","* Random Forest Classifier: Uses 100 Decision Trees but adds feature randomness.\n","* Evaluation: Compares test set accuracy of both models.\n","\n","**Expected Outcome:**\n","* Random Forest usually outperforms Bagging because it introduces additional randomness by selecting a random subset of features for each split.\n","* Bagging may perform similarly in some cases, depending on dataset characteristics."],"metadata":{"id":"ekLhILt2eqEm"}},{"cell_type":"markdown","source":["##Q 31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n","**Ans** - Python implementation to train a Random Forest Classifier on the Breast Cancer dataset and tune hyperparameters using GridSearchCV."],"metadata":{"id":"pLvD4477Domf"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import accuracy_score\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","    'criterion': ['gini', 'entropy']\n","}\n","\n","rf_clf = RandomForestClassifier(random_state=42)\n","\n","grid_search = GridSearchCV(rf_clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n","grid_search.fit(X_train, y_train)\n","\n","best_params = grid_search.best_params_\n","\n","best_rf_clf = RandomForestClassifier(**best_params, random_state=42)\n","best_rf_clf.fit(X_train, y_train)\n","\n","y_pred = best_rf_clf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(\"Best Hyperparameters:\", best_params)\n","print(f\"Best Random Forest Classifier Accuracy: {accuracy:.2f}\")"],"metadata":{"id":"xur152MCe5xx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset.\n","* GridSearchCV: Tunes multiple hyperparameters.\n","* Cross-Validation: Uses 5-fold CV to find the best hyperparameter combination.\n","* Final Model: Trains Random Forest with best hyperparameters and evaluates accuracy.\n","\n","**Expected Outcome:**\n","* Best Hyperparameters are printed.\n","* Optimized model should perform better than the default one."],"metadata":{"id":"AFdK7gfFe9Tq"}},{"cell_type":"markdown","source":["##Q 32. Train a Bagging Regressor with different numbers of base estimators and compare performance.\n","**Ans** - Python implementation to train a Bagging Regressor with different numbers of base estimators and compare performance using Mean Squared Error on the California Housing dataset."],"metadata":{"id":"3Z3OTjETDo8O"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","\n","data = fetch_california_housing()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","n_estimators_list = [1, 5, 10, 50, 100, 200]\n","mse_scores = []\n","\n","for n in n_estimators_list:\n","    bagging_reg = BaggingRegressor(\n","        base_estimator=DecisionTreeRegressor(),\n","        n_estimators=n,\n","        random_state=42\n","    )\n","\n","    bagging_reg.fit(X_train, y_train)\n","\n","    y_pred = bagging_reg.predict(X_test)\n","\n","    mse = mean_squared_error(y_test, y_pred)\n","    mse_scores.append(mse)\n","\n","    print(f\"Bagging Regressor (n_estimators={n}) MSE: {mse:.2f}\")\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(n_estimators_list, mse_scores, marker='o', linestyle='-')\n","plt.xlabel(\"Number of Base Estimators\")\n","plt.ylabel(\"Mean Squared Error (MSE)\")\n","plt.title(\"Bagging Regressor Performance vs. Number of Base Estimators\")\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"NcbTe2ehfPsh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the California Housing dataset.\n","* Bagging Regressor: Uses Decision Tree Regressors as base learners.\n","* Hyperparameter Variation: Tests with 1, 5, 10, 50, 100, 200 estimators.\n","* Evaluation: Computes MSE for each configuration.\n","* Visualization: Plots MSE vs. number of base estimators to observe performance trends.\n","\n","**Expected Outcome:**\n","* Increasing the number of estimators reduces MSE.\n","* Too many estimators may lead to diminishing returns in performance improvement.\n","* Plot helps identify the optimal number of base estimators."],"metadata":{"id":"0V1mZOalfTTh"}},{"cell_type":"markdown","source":["##Q 33. Train a Random Forest Classifier and analyze misclassified samples.\n","**Ans** - Python implementation to train a Random Forest Classifier on the Breast Cancer dataset and analyze misclassified samples by comparing predictions with actual labels."],"metadata":{"id":"1Ym9ClgCDpQW"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","feature_names = data.feature_names\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_clf.fit(X_train, y_train)\n","\n","y_pred = rf_clf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Random Forest Classifier Accuracy: {accuracy:.2f}\\n\")\n","\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n","\n","misclassified_idx = np.where(y_pred != y_test)[0]\n","\n","misclassified_samples = pd.DataFrame(X_test[misclassified_idx], columns=feature_names)\n","misclassified_samples['Actual Label'] = y_test[misclassified_idx]\n","misclassified_samples['Predicted Label'] = y_pred[misclassified_idx]\n","\n","print(\"\\nMisclassified Samples:\")\n","print(misclassified_samples)"],"metadata":{"id":"c5kcWJddfpnp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset.\n","* Random Forest Classifier: Trains 100 trees for classification.\n","* Evaluation: Computes accuracy and confusion matrix to check performance.\n","* Misclassified Samples:\n","  * Extracts test samples where predictions differ from actual labels.\n","  * Creates a DataFrame showing feature values, actual labels, and predicted labels.\n","\n","**Expected Outcome:**\n","* Accuracy score is printed.\n","* Confusion matrix shows false positives & false negatives.\n","* Misclassified samples list helps analyze where the model struggles."],"metadata":{"id":"hFOnPxSlfte6"}},{"cell_type":"markdown","source":["##Q 34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n","**Ans** - Python implementation to train a Bagging Classifier and compare its performance with a single Decision Tree Classifier on the Breast Cancer dataset using accuracy as the evaluation metric."],"metadata":{"id":"ouCk5p1RDpl_"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","dt_clf = DecisionTreeClassifier(random_state=42)\n","dt_clf.fit(X_train, y_train)\n","\n","bagging_clf = BaggingClassifier(\n","    base_estimator=DecisionTreeClassifier(),\n","    n_estimators=100,\n","    random_state=42\n",")\n","bagging_clf.fit(X_train, y_train)\n","\n","y_pred_dt = dt_clf.predict(X_test)\n","y_pred_bagging = bagging_clf.predict(X_test)\n","\n","accuracy_dt = accuracy_score(y_test, y_pred_dt)\n","accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n","\n","print(f\"Decision Tree Classifier Accuracy: {accuracy_dt:.2f}\")\n","print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")\n","\n","if accuracy_bagging > accuracy_dt:\n","    print(\"Bagging Classifier performs better!\")\n","elif accuracy_dt > accuracy_bagging:\n","    print(\"Single Decision Tree performs better!\")\n","else:\n","    print(\"Both models have similar accuracy!\")"],"metadata":{"id":"cz-on6S1gGth"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset.\n","* Single Decision Tree: Trains a Decision Tree Classifier on the dataset.\n","* Bagging Classifier: Uses 100 Decision Trees to improve stability and accuracy.\n","* Evaluation: Compares test set accuracy of both models.\n","\n","**Expected Outcome:**\n","* Bagging Classifier usually performs better because it reduces variance and improves generalization.\n","* Single Decision Tree may overfit, leading to lower test accuracy."],"metadata":{"id":"nBrSeZY4gKS6"}},{"cell_type":"markdown","source":["##Q 35. Train a Random Forest Classifier and visualize the confusion matrix.\n","**Ans** - Python implementation to train a Random Forest Classifier on the Breast Cancer dataset and visualize the confusion matrix using Seaborn heatmap."],"metadata":{"id":"RIySTLLjDp5-"}},{"cell_type":"code","source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_clf.fit(X_train, y_train)\n","\n","y_pred = rf_clf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Random Forest Classifier Accuracy: {accuracy:.2f}\\n\")\n","\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title('Confusion Matrix - Random Forest Classifier')\n","plt.show()"],"metadata":{"id":"6H1KnpbPgZMk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset (binary classification: Benign (0) vs. Malignant (1)).\n","* Random Forest Classifier: Uses 100 trees to train the model.\n","* Confusion Matrix:\n","  * Rows: Actual labels\n","  * Columns: Predicted labels\n","  * Diagonal values: Correct predictions\n","  * Off-diagonal values: Misclassifications\n","* Visualization: Uses Seaborn heatmap to display the confusion matrix clearly.\n","\n","**Expected Outcome:**\n","* High accuracy for this dataset.\n","* Most values should be on the diagonal, indicating correct classifications.\n","* Misclassified samples can be analyzed further if needed."],"metadata":{"id":"xxcA5ZsIgc7Z"}},{"cell_type":"markdown","source":["##Q 36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n","**Ans** - Python implementation to train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression as base estimators and Logistic Regression as the final meta-classifier."],"metadata":{"id":"WB3pZpndDqLv"}},{"cell_type":"code","source":["from sklearn.ensemble import StackingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","base_estimators = [\n","    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n","    ('svm', SVC(probability=True, random_state=42)),\n","    ('log_reg', LogisticRegression(max_iter=1000, random_state=42))\n","]\n","\n","stacking_clf = StackingClassifier(\n","    estimators=base_estimators,\n","    final_estimator=LogisticRegression(max_iter=1000, random_state=42)\n",")\n","\n","stacking_clf.fit(X_train, y_train)\n","\n","y_pred = stacking_clf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Stacking Classifier Accuracy: {accuracy:.2f}\")\n","\n","models = {\n","    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n","    \"SVM\": SVC(random_state=42),\n","    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42)\n","}\n","\n","for name, model in models.items():\n","    model.fit(X_train, y_train)\n","    y_pred_model = model.predict(X_test)\n","    acc = accuracy_score(y_test, y_pred_model)\n","    print(f\"{name} Accuracy: {acc:.2f}\")\n","\n"],"metadata":{"id":"UcDbjjjvgy6J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset.\n","* Base Models:\n","  * Decision Tree\n","  * Support Vector Machine\n","  * Logistic Regression\n","    * Final Meta-Classifier: Logistic Regression combines base models' predictions.\n","    * Evaluation:\n","* Compares stacking accuracy with individual models.\n","* Prints accuracy scores for all models.\n","\n","**Expected Outcome:**\n","* Stacking Classifier usually outperforms individual models by combining strengths.\n","* SVM, Decision Tree, and Logistic Regression accuracies vary based on dataset characteristics.\n","* If individual models have low variance, stacking may show only slight improvement."],"metadata":{"id":"I5FrXvoTg2vZ"}},{"cell_type":"markdown","source":["##Q 37. Train a Random Forest Classifier and print the top 5 most important features.\n","**Ans** - Python implementation to train a Random Forest Classifier on the Breast Cancer dataset and print the top 5 most important features based on feature importance scores."],"metadata":{"id":"-fCRDoi0DqpW"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","feature_names = data.feature_names\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_clf.fit(X_train, y_train)\n","\n","feature_importances = rf_clf.feature_importances_\n","\n","feature_importance_df = pd.DataFrame({\n","    'Feature': feature_names,\n","    'Importance': feature_importances\n","}).sort_values(by='Importance', ascending=False)\n","\n","print(\"Top 5 Most Important Features:\")\n","print(feature_importance_df.head(5))"],"metadata":{"id":"kunGwruihXNL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset.\n","* Random Forest Classifier: Uses 100 trees for training.\n","* Feature Importance:\n","  * Extracts feature importance scores from the trained model.\n","  * Sorts them in descending order to identify the most influential features.\n","* Displays the top 5 features contributing the most to predictions.\n","\n","**Expected Outcome:**\n","* Prints the top 5 features along with their importance scores.\n","* Helps in feature selection by identifying the most relevant attributes."],"metadata":{"id":"eD2iSKEZha-S"}},{"cell_type":"markdown","source":["##Q 38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n","**Ans** - Python implementation to train a Bagging Classifier on the Breast Cancer dataset and evaluate its performance using Precision, Recall, and F1-score."],"metadata":{"id":"bAGBc2GXDq8n"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","bagging_clf = BaggingClassifier(\n","    base_estimator=DecisionTreeClassifier(),\n","    n_estimators=100,\n","    random_state=42\n",")\n","bagging_clf.fit(X_train, y_train)\n","\n","y_pred = bagging_clf.predict(X_test)\n","\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Bagging Classifier Performance:\")\n","print(f\"Accuracy  : {accuracy:.2f}\")\n","print(f\"Precision : {precision:.2f}\")\n","print(f\"Recall    : {recall:.2f}\")\n","print(f\"F1-score  : {f1:.2f}\")"],"metadata":{"id":"sykmj5ORhvM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset (binary classification: Benign (0) vs. Malignant (1)).\n","* Bagging Classifier: Uses 100 Decision Trees to improve stability and accuracy.\n","* Evaluation Metrics:\n","  * Precision: Measures the proportion of true positives among predicted positives.\n","  * Recall: Measures the proportion of true positives among actual positives.\n","  * F1-score: Harmonic mean of Precision & Recall, useful for imbalanced datasets.\n","  * Accuracy: Overall correctness of the model.\n","\n","**Expected Outcome:**\n","* High Precision (~0.95+) ensures fewer false positives.\n","* High Recall (~0.95+) ensures fewer false negatives.\n","* F1-score balances both precision & recall.\n","* Accuracy is usually above 90% for this dataset."],"metadata":{"id":"XIV4Zy5vhy8b"}},{"cell_type":"markdown","source":["##Q 39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n","**Ans** - Python implementation to train a Random Forest Classifier on the Breast Cancer dataset and analyze the effect of max_depth on accuracy."],"metadata":{"id":"bX07vrFWDrRX"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","max_depth_values = [2, 4, 6, 8, 10, 15, 20, None]\n","accuracy_scores = []\n","\n","for max_depth in max_depth_values:\n","    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n","    rf_clf.fit(X_train, y_train)\n","    y_pred = rf_clf.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    accuracy_scores.append(accuracy)\n","    print(f\"Max Depth: {max_depth}, Accuracy: {accuracy:.2f}\")\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot([str(md) for md in max_depth_values], accuracy_scores, marker='o', linestyle='-')\n","plt.xlabel('Max Depth')\n","plt.ylabel('Accuracy')\n","plt.title('Effect of Max Depth on Random Forest Accuracy')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"dcKVgWtIiLPs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset (binary classification).\n","* Random Forest Classifier: Trains with different values of max_depth.\n","* Effect of max_depth:\n","  * Too small (underfitting): Low accuracy.\n","  * Too large (overfitting): Accuracy might decrease.\n","  * Optimal depth: A balance between bias and variance.\n","    * Visualization: Plots max_depth vs. accuracy to show the impact clearly.\n","\n","**Expected Outcome:**\n","* Shallow trees (low max_depth) may underfit and have lower accuracy.\n","* Deep trees (high max_depth) may overfit but still achieve high accuracy.\n","* Somewhere in between (like max_depth=6 to 10) is often optimal for this dataset."],"metadata":{"id":"bueFnIXAiPSk"}},{"cell_type":"markdown","source":["##Q 40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n","**Ans** - Python implementation to train a Bagging Regressor using two different base estimators:\n","\n","1. Decision Tree Regressor\n","2. K-Neighbors Regressor\n","\n","The models are evaluated using Mean Squared Error for comparison.\n","\n","**Implementation:**"],"metadata":{"id":"wYiI_5pzDrk4"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import BaggingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","X, y = make_regression(n_samples=500, n_features=1, noise=20, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","dt_regressor = DecisionTreeRegressor(random_state=42)\n","knn_regressor = KNeighborsRegressor(n_neighbors=5)\n","\n","bagging_dt = BaggingRegressor(base_estimator=dt_regressor, n_estimators=50, random_state=42)\n","bagging_dt.fit(X_train, y_train)\n","y_pred_dt = bagging_dt.predict(X_test)\n","mse_dt = mean_squared_error(y_test, y_pred_dt)\n","\n","bagging_knn = BaggingRegressor(base_estimator=knn_regressor, n_estimators=50, random_state=42)\n","bagging_knn.fit(X_train, y_train)\n","y_pred_knn = bagging_knn.predict(X_test)\n","mse_knn = mean_squared_error(y_test, y_pred_knn)\n","\n","print(f\"Bagging Regressor (Decision Tree) MSE: {mse_dt:.2f}\")\n","print(f\"Bagging Regressor (K-Neighbors) MSE: {mse_knn:.2f}\")\n","\n","plt.figure(figsize=(10, 5))\n","plt.scatter(X_test, y_test, color='gray', alpha=0.5, label=\"True Values\")\n","plt.scatter(X_test, y_pred_dt, color='blue', label=\"Decision Tree (Bagging)\", alpha=0.6)\n","plt.scatter(X_test, y_pred_knn, color='red', label=\"K-Neighbors (Bagging)\", alpha=0.6)\n","plt.legend()\n","plt.xlabel(\"Feature\")\n","plt.ylabel(\"Target\")\n","plt.title(\"Comparison of Bagging Regressors with Different Base Estimators\")\n","plt.show()"],"metadata":{"id":"paqWVnEEi3Mb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses a synthetic regression dataset with noise to simulate real-world variations.\n","* Base Models:\n","  * Decision Tree Regressor: High variance, captures complex patterns.\n","  * K-Neighbors Regressor: Based on nearest neighbors, smoother predictions.\n","    * Evaluation Metric:\n","  * Mean Squared Error (MSE): Lower value = better performance.\n","    * Visualization:\n","  * Gray points: Actual data.\n","  * Blue (Decision Tree) & Red (KNN) points: Predictions from respective models.\n","\n","**Expected Outcome:**\n","* Decision Tree (Bagging) usually performs well on non-linear data.\n","* K-Neighbors (Bagging) may be smoother but less flexible in complex cases.\n","* Lower MSE indicates a better model fit."],"metadata":{"id":"_CR3My44i8FU"}},{"cell_type":"markdown","source":["##Q 41.Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n","**Ans** - Python implementation to train a Random Forest Classifier on the Breast Cancer dataset and evaluate its performance using the ROC-AUC Score.\n","\n","**Implementation:**"],"metadata":{"id":"C7YbxyErDr8e"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score, roc_curve\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_clf.fit(X_train, y_train)\n","\n","y_probs = rf_clf.predict_proba(X_test)[:, 1]\n","\n","roc_auc = roc_auc_score(y_test, y_probs)\n","print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n","\n","fpr, tpr, _ = roc_curve(y_test, y_probs)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f\"Random Forest (AUC = {roc_auc:.2f})\", color='blue')\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","plt.xlabel(\"False Positive Rate (FPR)\")\n","plt.ylabel(\"True Positive Rate (TPR)\")\n","plt.title(\"ROC Curve for Random Forest Classifier\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"thdLdfO-jdmV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset for binary classification.\n","* Random Forest Classifier: Trains a model with 100 trees for improved accuracy.\n","* ROC-AUC Score:\n","  * Measures classifier performance between 0 and 1 (higher is better).\n","  * Closer to 1 means better distinction between positive and negative classes.\n","    * ROC Curve:\n","* Plots TPR (Sensitivity) vs. FPR at different classification thresholds.\n","* Higher area under curve (AUC) = better performance.\n","\n","**Expected Outcome:**\n","* ROC-AUC Score should be high (~0.95+), indicating strong classification.\n","* ROC Curve should be close to the top-left corner, meaning fewer false positives."],"metadata":{"id":"u7BNWwaKjhxr"}},{"cell_type":"markdown","source":["##Q 42. Train a Bagging Classifier and evaluate its performance using cross-validation.\n","**Ans** - Python implementation to train a Bagging Classifier on the Breast Cancer dataset and evaluate its performance using cross-validation.\n","\n","**Implementation:**"],"metadata":{"id":"DCD2D9MMDsTA"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","bagging_clf = BaggingClassifier(\n","    base_estimator=DecisionTreeClassifier(),\n","    n_estimators=100,\n","    random_state=42\n",")\n","\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","cv_scores = cross_val_score(bagging_clf, X, y, cv=cv, scoring='accuracy')\n","\n","print(f\"Cross-Validation Scores: {cv_scores}\")\n","print(f\"Mean Accuracy: {np.mean(cv_scores):.2f}\")\n","print(f\"Standard Deviation: {np.std(cv_scores):.2f}\")"],"metadata":{"id":"0EGBEBzlj5Zy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset (binary classification).\n","* Bagging Classifier: Uses 100 Decision Trees to improve stability and accuracy.\n","* Cross-Validation:\n","  * Uses 5-fold Stratified K-Fold CV to ensure balanced class distribution.\n","  * Computes model accuracy across different folds.\n","* Performance Metrics:\n","  * Mean Accuracy: Measures overall model performance.\n","  * Standard Deviation: Measures model consistency.\n","\n","**Expected Outcome:**\n","* High Mean Accuracy (~0.94+), indicating strong classification.\n","* Low Standard Deviation, meaning consistent performance across different folds."],"metadata":{"id":"gbdUuIExj-qq"}},{"cell_type":"markdown","source":["##Q 43. Train a Random Forest Classifier and plot the Precision-Recall curve.\n","**Ans** - Python implementation to train a Random Forest Classifier on the Breast Cancer dataset and plot the Precision-Recall Curve.\n","\n","**Implementation:**"],"metadata":{"id":"Eo0vnu0ADso4"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_clf.fit(X_train, y_train)\n","\n","y_probs = rf_clf.predict_proba(X_test)[:, 1]\n","\n","precision, recall, _ = precision_recall_curve(y_test, y_probs)\n","\n","average_precision = average_precision_score(y_test, y_probs)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, label=f\"Random Forest (AP = {average_precision:.2f})\", color='blue')\n","plt.xlabel(\"Recall\")\n","plt.ylabel(\"Precision\")\n","plt.title(\"Precision-Recall Curve for Random Forest Classifier\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"x3FTOmCTkUVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset for binary classification.\n","* Random Forest Classifier: Trains a model with 100 trees for robust classification.\n","* Precision-Recall Curve:\n","  * Precision: Measures how many predicted positives are actually correct.\n","  * Recall: Measures how many actual positives are correctly predicted.\n","* Average Precision Score (AP):\n","  * Higher AP means better performance at handling imbalanced data.\n","  * Useful when classes are imbalanced (e.g., rare disease detection).\n","\n","**Expected Outcome:**\n","* Precision-Recall curve should be smooth and high (indicating strong classification).\n","* Higher AP (~0.95+) suggests the model is good at distinguishing between classes."],"metadata":{"id":"EA5jK59lkXpB"}},{"cell_type":"markdown","source":["##Q 44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n","**Ans** - Python implementation to train a Stacking Classifier using Random Forest and Logistic Regression as base estimators and Logistic Regression as the final meta-classifier. We'll then compare accuracy using cross-validation.\n","\n","**Implementation**"],"metadata":{"id":"UapCzIEPDtAW"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import accuracy_score\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","base_models = [\n","    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n","    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n","]\n","\n","stacking_clf = StackingClassifier(\n","    estimators=base_models,\n","    final_estimator=LogisticRegression(),\n","    passthrough=False\n",")\n","\n","stacking_clf.fit(X_train, y_train)\n","\n","y_pred = stacking_clf.predict(X_test)\n","\n","stacking_accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Stacking Classifier Accuracy: {stacking_accuracy:.2f}\")\n","\n","rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n","\n","rf_cv_score = np.mean(cross_val_score(rf_clf, X, y, cv=5, scoring='accuracy'))\n","lr_cv_score = np.mean(cross_val_score(lr_clf, X, y, cv=5, scoring='accuracy'))\n","stacking_cv_score = np.mean(cross_val_score(stacking_clf, X, y, cv=5, scoring='accuracy'))\n","\n","print(f\"Random Forest CV Accuracy: {rf_cv_score:.2f}\")\n","print(f\"Logistic Regression CV Accuracy: {lr_cv_score:.2f}\")\n","print(f\"Stacking Classifier CV Accuracy: {stacking_cv_score:.2f}\")"],"metadata":{"id":"zC50fJtJkwNS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","* Dataset: Uses the Breast Cancer dataset for binary classification.\n","* Stacking Classifier Setup:\n","* Base models:\n","  * Random Forest Classifier (good for capturing complex relationships).\n","  * Logistic Regression (performs well on linear relationships).\n","* Meta-classifier:\n","  * Logistic Regression (takes base model outputs and makes final prediction).\n","  * Performance Comparison:\n","* Uses cross-validation (5-fold) to compare individual models vs. stacking.\n","\n","**Expected Outcome:**\n","* Stacking Classifier should have higher accuracy than individual models.\n","* Random Forest may perform better than Logistic Regression, but stacking can improve overall performance."],"metadata":{"id":"nqbRA8wlk33P"}},{"cell_type":"markdown","source":["##Q 45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n","**Ans** - Python implementation to train a Bagging Regressor using different levels of bootstrap samples and compare performance using Mean Squared Error (MSE).\n","\n","**Implementation:**"],"metadata":{"id":"jsCUvqKVDtVG"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import BaggingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","X, y = make_regression(n_samples=500, n_features=10, noise=0.2, random_state=42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","bootstrap_levels = [True, False]\n","mse_scores = {}\n","\n","for bootstrap in bootstrap_levels:\n","    bagging_reg = BaggingRegressor(\n","        base_estimator=DecisionTreeRegressor(),\n","        n_estimators=100,\n","        bootstrap=bootstrap,\n","        random_state=42\n","    )\n","\n","    bagging_reg.fit(X_train, y_train)\n","    y_pred = bagging_reg.predict(X_test)\n","\n","    mse = mean_squared_error(y_test, y_pred)\n","    mse_scores[f\"Bootstrap={bootstrap}\"] = mse\n","    print(f\"Bagging Regressor (Bootstrap={bootstrap}) - MSE: {mse:.2f}\")\n","\n","plt.figure(figsize=(8, 5))\n","plt.bar(mse_scores.keys(), mse_scores.values(), color=['blue', 'orange'])\n","plt.xlabel(\"Bootstrap Sampling\")\n","plt.ylabel(\"Mean Squared Error (MSE)\")\n","plt.title(\"Effect of Bootstrap Sampling on Bagging Regressor Performance\")\n","plt.show()"],"metadata":{"id":"KghGPJmnlSEa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interpretation**\n","* Lower max_samples (e.g., 0.3) may lead to underfitting due to less data per estimator.\n","* Higher max_samples (e.g., 1.0) might reduce bias but increase variance.\n","* There's often a sweet spot (e.g., 0.7-1.0) depending on the dataset.\n","\n","**Explanation:**\n","* Dataset: Uses a synthetic regression dataset with noise to simulate real-world conditions.\n","* Bagging Regressor: Uses 100 Decision Trees as base learners.\n","* Bootstrap Sampling:\n","  * True → Each base estimator trains on randomly drawn samples with replacement.\n","  * False → Each base estimator trains on a random subset without replacement.\n","    * Performance Metric: Mean Squared Error (MSE)\n","  * Lower MSE means better predictions.\n","\n","**Expected Outcome:**\n","* Bootstrap=True typically performs better since each model gets diverse training data.\n","* Bootstrap=False may overfit more but can sometimes work well on smaller datasets.\n","* The bar chart helps visualize the impact of bootstrap sampling on model performance."],"metadata":{"id":"YzqHsY5elW1S"}}]}